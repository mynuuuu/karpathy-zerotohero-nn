# -*- coding: utf-8 -*-
"""makemore1

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1F17-dq5o6WGkFwqt8vbd6KSCx7_GmGVU
"""

words = open('names.txt', 'r').read().splitlines()

words[:10]

len(words)

min(len(w) for w in words)

max(len(w) for w in words)

"""Bigram -> when one char is given, predicting what comes next"""

b = {}
for w in words[:3]:
  # The zip function pairs elements from two iterables.
  # w: 'emma' -> 'e', 'm', 'm', 'a'
  # w[1:]: 'mma' -> 'm', 'm', 'a'
  # So, zip(w, w[1:]) will produce pairs: ('e', 'm'), ('m', 'm'), ('m', 'a')

  chs = ['.'] + list(w) + ['.']
  for ch1, ch2 in zip(chs, chs[1:]):
    bigram = (ch1, ch2)
    b[bigram] = b.get(bigram, 0) + 1

# b.items() gives (key, value) pairs from the dictionary
# Example: (('e','m'), 5)

sorted(
    b.items(),                   # list of (bigram, count)
    key=lambda kv: -kv[1]         # sort by NEGATIVE count (descending order)
)

import torch

N = torch.zeros((28,28), dtype=torch.int32)

chars = sorted(list(set(''.join(words))))
stoi = {s:i+1 for i,s in enumerate(chars)}
stoi['.'] = 0
itos = {i:s for s,i in stoi.items()}
itos

for w in words:
  chs = ['.'] + list(w) + ['.']
  for ch1, ch2 in zip(chs, chs[1:]):
    ix1 = stoi[ch1]
    ix2 = stoi[ch2]
    N[ix1, ix2] += 1

# Commented out IPython magic to ensure Python compatibility.
import matplotlib.pyplot as plt
# %matplotlib inline

plt.figure(figsize=(16,16))
plt.imshow(N, cmap = 'Blues')
for i in range(27):
  for j in range(27):
    chstr = itos[i] + itos[j]
    plt.text(j, i, chstr, ha='center', va='bottom', color='black')
    plt.text(j, i, N[i,j].item(), ha='center', va='top', color='black')
plt.axis('off')

N[0]

p = N[0].float()
p = p/p.sum()
p

g = torch.Generator().manual_seed(2149837)
ix = torch.multinomial(p, num_samples=1, replacement=True, generator=g).item()
itos[ix]

p.shape

#P.shape
#P.sum(1, keepdim=True).shape

# Convert counts to float so we can safely divide
P = (N+1).float()

# Compute row-wise sums:
# P.sum(1) sums across columns for each row
# keepdim=True keeps the shape as (27, 1) instead of (27,)
# so each row has ONE number representing its total count
row_sums = P.sum(1, keepdim=True)

# Broadcasting happens here:
# P has shape (27, 27)
# row_sums has shape (27, 1)
# PyTorch "stretches" (27, 1) -> (27, 27) across columns
# so each row in P is divided by its own row sum
P = P / row_sums

"""
Broadcasting explanation (the "stretching" part):

P has shape (27, 27)
row_sums has shape (27, 1)

row_sums looks like this conceptually:
[
  [s0],
  [s1],
  [s2],
  ...
  [s26]
]

PyTorch sees the dimension '1' in (27, 1) and understands:
"This single value per row should be reused across all 27 columns"

So during division, PyTorch *pretends* row_sums is:
[
  [s0, s0, s0, ..., s0],
  [s1, s1, s1, ..., s1],
  [s2, s2, s2, ..., s2],
  ...
  [s26, s26, s26, ..., s26]
]

IMPORTANT:
- This stretched matrix is NOT actually created in memory
- It is a virtual expansion done internally for efficiency

Result:
Each element in a row of P is divided by the SAME row sum,
which normalizes that row so it sums to 1 (valid probabilities).
"""

g = torch.Generator().manual_seed(2149837)

for i in range(5):
  out = []
  ix = 0
  while True:
    p = P[ix]
    ix = torch.multinomial(p, num_samples=1, replacement=True, generator=g).item()
    out.append(itos[ix])
    if ix == 0:
      break
  print(''.join(out))

#GOAL: maximize th elikelihoood of the data w.r.t. th model parameters (statistical modelling)
#equivalent to maximizing the log likelihood (because log is. monotonic)
#equivalent to minimizing the negative log likelihood
#equivalent to minimizing the average of negative log likelihood

#log(a*b*c) = log(a) + log(b) + log(c)

likelihood = 0
log_likelihood = 0
n = 0
#for w in 'swetha':
for w in words[:3]:
  chs = ['.'] + list(w) + ['.']
  for ch1, ch2 in zip(chs, chs[1:]):
    ix1 = stoi[ch1]
    ix2 = stoi[ch2]
    N[ix1, ix2] += 1
    prob = P[ix1,ix2]
    likelihood *= prob
    logprob = torch.log(prob)
    log_likelihood += logprob
    n += 1
    print(f"{ch1}{ch2}: {prob:.4f} {logprob:.4f}")

print(f'{likelihood=:.20f}')
print(f'{log_likelihood=}')
nll = -log_likelihood
print(f'{nll=}')
print(f'{nll/n}')

#create a training set of al the bigrams (x, y)
xs, ys = [], []

for w in words[:1]:
  chs = ['.'] + list(w) + ['.']
  for ch1, ch2 in zip(chs, chs[1:]):
    ix1 = stoi[ch1]
    ix2 = stoi[ch2]
    print(ch1, ch2)
    xs.append(ix1)
    ys.append(ix2)

xs = torch.tensor(xs)
ys = torch.tensor(ys)

xs

ys

import torch.nn.functional as F
xenc = F.one_hot(xs, num_classes=27).float() #cast to float to feed into neural nets
xenc

W = torch.randn((27,27))
xenc @ W #it represents matrix multiplication
#output shape is (5,27) because it is a product of (5, 27) * (27, 27)

# W has shape (27, 27)
#
# Interpretation of dimensions:
# - Rows (first 27): represent the CURRENT character
# - Columns (second 27): represent the POSSIBLE NEXT characters
#
# So W[i, j] is a learnable score (logit) for:
# "How likely is character j to come after character i?"
#
# You can think of W as a big table:
#   - each row corresponds to one input character
#   - each row contains 27 numbers (one for each possible next character)
#
# When we multiply a one-hot encoded input xenc with W:
#   xenc.shape = (batch_size, 27)
#   W.shape    = (27, 27)
#
# xenc @ W selects ONE FULL ROW of W for each input character
# (because one-hot vectors have a single 1 and the rest 0s)
#
# The result has shape (batch_size, 27) and represents:
# - 27 logits (scores) for the next character
# - one score per possible next character
#
# These logits are later passed through softmax to become probabilities

logits = (xenc @ W) #log - counts
counts = logits.exp() #equivalent N
prob = counts / counts.sum(1, keepdim=True)
prob

#----------> SUMMARY

xs

ys

#randomly initialize 27 neurons' weights, each neuron receives 27 inputs
g = torch.Generator().manual_seed(2147483647)
W = torch.randn((27,27), generator=g)

xenc = F.one_hot(xs, num_classes=27).float()
logits = xenc @ W
counts = logits.exp() #equivalent N
prob = counts / counts.sum(1, keepdim=True)
#the last two lines are together called softmax

prob.shape

nlls = torch.zeros(5)
for i in range(5):
  #i-th bigram:
  x = xs[i].item()
  y = ys[i].item()
  print('------')
  print(f'bigram example {i+1}: {itos[x]}{itos[y]} (indexes {x},{y})')
  print(f'Input to the neural net: {x}')
  print(f'output probabilities from the neural net:', prob[i])
  print(f'label (actual next character): {y}')
  p = prob[i, y]
  print(f'the probability assigned by neural net to the actual char y: {p.item()}')
  logp = torch.log(p)
  print(f'log likelihood: {logp}')
  nll = -logp
  print(f'negative log likelihood: {nll}')
  nlls[i] = nll

print('========')
print('average nll, loss = ', nlls.mean().item())

#-------->OPTIMISATION

xs

ys

from numpy import require
#randomly initialize 27 neurons' weights, each neuron receives 27 inputs
g = torch.Generator().manual_seed(2147483647)
W = torch.randn((27,27), generator=g, requires_grad=True)

#forward pass
xenc = F.one_hot(xs, num_classes=27).float()
logits = xenc @ W
counts = logits.exp() #equivalent N
prob = counts / counts.sum(1, keepdim=True)
loss = -prob[torch.arange(5), ys].log().mean()

"""
# we need - prob[0,5], prob[2,13], prob[2,13], prob[3,1], prob[4,0]
# this can be done in pytorch like this
print(torch.arange(5))
print(prob[torch.arange(5), ys])
"""

print(loss.item())

#backward pass
W.grad = None
loss.backward()

W.data += -0.1 * W.grad

#------> ORGANIZED VERSION

#create a training set of al the bigrams (x, y)
xs, ys = [], []
for w in words:
  chs = ['.'] + list(w) + ['.']
  for ch1, ch2 in zip(chs, chs[1:]):
    ix1 = stoi[ch1]
    ix2 = stoi[ch2]
    #print(ch1, ch2)
    xs.append(ix1)
    ys.append(ix2)
xs = torch.tensor(xs)
ys = torch.tensor(ys)
num = xs.nelement()
print('number of examples: ', num)

#randomly initialize 27 neurons' weights, each neuron receives 27 inputs
g = torch.Generator().manual_seed(2147483647)
W = torch.randn((27,27), generator=g, requires_grad=True)

#gradient descent
for i in range(200):
  #forward pass
  xenc = F.one_hot(xs, num_classes=27).float()
  logits = xenc @ W
  counts = logits.exp() #equivalent N
  prob = counts / counts.sum(1, keepdim=True)
  loss = -prob[torch.arange(num), ys].log().mean()
  print(loss.item())

  #backward pass
  W.grad = None
  loss.backward()

  #update
  W.data += -50 * W.grad

